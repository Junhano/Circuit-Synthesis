{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a42103b-47be-40e3-9d64-13d4595d50c6",
   "metadata": {},
   "source": [
    "# Probablistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678db546-b1d7-43ac-b907-cf30cf664d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from Training import model, utils, dataset, train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defc55b7-af20-44be-90f1-f39f0853dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc(y_hat,y,margin=0.05):\n",
    "    a_err = (np.abs(y_hat - y)) # get normalized error \n",
    "    err = np.divide(a_err, y, out=a_err, where=y==0)\n",
    "    assert(err.shape == y.shape)\n",
    "    num_correct = 0\n",
    "    for row in err:\n",
    "        num_in_row = len(np.where(row < margin)[0]) # margin * 100 because \n",
    "        if num_in_row == len(row):\n",
    "            num_correct += 1\n",
    "\n",
    "    num_samples = y.shape[0]\n",
    "    correct_idx = np.where(err < margin)\n",
    "    num_part_correct = len(correct_idx[0])\n",
    "    num_part_samples = y.shape[0] * y.shape[1]\n",
    "    print(f\"Correct = {num_correct} / {num_samples}\")\n",
    "    return (num_correct/num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7398448-40f9-4c10-afb8-69a8444511c0",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d2858d-a42f-48f6-9de3-d26d88247ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.parseGainAndBWCsv2(\"Data/BW-3000.csv\").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53cbbc10-eb8d-490a-848c-23832f79bc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.200e+02 2.880e-06 3.775e+00 8.280e+10]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7ae70-e420-4c5f-8712-a0dc166fb873",
   "metadata": {},
   "source": [
    "##### Split data into inputs X and target Y Normalize to [0,1] (inclusive) and create Training and Testign Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd2625d6-6c84-44b0-9b60-68e46eeb41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocessing.MinMaxScaler((0,1)).fit_transform(data)\n",
    "X = data[:,:2]\n",
    "Y = data[:,2:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25750082-6130-442e-929d-67ce5e824bb7",
   "metadata": {},
   "source": [
    "#### Define Model and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c611a-c7f2-4170-94af-4d2025de4f27",
   "metadata": {},
   "source": [
    "##### Define model\n",
    "\n",
    "* Model: MLP with variable hidden layer number and width  \n",
    "* Output layer: \n",
    "    * Node 1 and 2 are the mean for probability mean \n",
    "    * Node 3 and 4 are the diagonal of the covariance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499f1200-7d1d-4910-a2ac-83f2852d4f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_hidden_layers = 5\n",
    "# hidden_layer_size = 500\n",
    "\n",
    "# model = tf.keras.models.Sequential([])\n",
    "\n",
    "# for i in range(num_hidden_layers):\n",
    "#     model.add(tf.keras.layers.Dense(hidden_layer_size, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(4, activation='sigmoid')) # add output layer: node 1,2 are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3618fa3-b062-454c-a941-d2d8783d52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = model.Sigmoid_Net(2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438de8d-1aae-4213-b15a-f70a13b80002",
   "metadata": {},
   "source": [
    "##### Define Loss\n",
    "\n",
    "* Loss: guassian_nll\n",
    "    * cite: https://gist.github.com/sergeyprokudin/4a50bf9b75e0559c1fcd2cae860b879e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "149403d4-2f8c-4238-9cb0-8c7ae7d36a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(test_model.parameters(),lr=0.01)\n",
    "loss_fn = nn.GaussianNLLLoss()\n",
    "\n",
    "dataset1 = dataset.CircuitSynthesisGainAndBandwidthManually(Y, X)\n",
    "train_dataset, val_dataset = utils.splitDataset(dataset1, 0.8)\n",
    "    \n",
    "train_data = DataLoader(train_dataset,batch_size = 500)\n",
    "validation_data = DataLoader(val_dataset, batch_size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8827726e-cbf1-4d93-8aea-51d8bfe01a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 10, loss = -0.7403\n",
      "t = 20, loss = -0.7424\n",
      "t = 30, loss = -0.7424\n",
      "t = 40, loss = -0.7424\n",
      "t = 50, loss = -0.7424\n",
      "t = 60, loss = -0.7424\n",
      "t = 70, loss = -0.7424\n",
      "t = 80, loss = -0.7424\n",
      "t = 90, loss = -0.7424\n",
      "t = 100, loss = -0.7424\n",
      "t = 110, loss = -0.7424\n",
      "t = 120, loss = -0.7459\n",
      "t = 130, loss = -0.7860\n",
      "t = 140, loss = -0.8464\n",
      "t = 150, loss = -1.0262\n",
      "t = 160, loss = -1.1248\n",
      "t = 170, loss = -1.1337\n",
      "t = 180, loss = -1.1442\n",
      "t = 190, loss = -1.1498\n",
      "t = 200, loss = -1.1491\n",
      "t = 210, loss = -1.1557\n",
      "t = 220, loss = -1.1464\n",
      "t = 230, loss = -1.1586\n",
      "t = 240, loss = -1.1604\n",
      "t = 250, loss = -1.1608\n",
      "t = 260, loss = -1.1597\n",
      "t = 270, loss = -1.1622\n",
      "t = 280, loss = -1.1629\n",
      "t = 290, loss = -1.1628\n",
      "t = 300, loss = -1.1633\n",
      "t = 310, loss = -1.1636\n",
      "t = 320, loss = -1.1614\n",
      "t = 330, loss = -1.1641\n",
      "t = 340, loss = -1.1638\n",
      "t = 350, loss = -1.1650\n",
      "t = 360, loss = -1.1634\n",
      "t = 370, loss = -1.1643\n",
      "t = 380, loss = -1.1649\n",
      "t = 390, loss = -1.1651\n",
      "t = 400, loss = -1.1651\n",
      "t = 410, loss = -1.1649\n",
      "t = 420, loss = -1.1642\n",
      "t = 430, loss = -1.1645\n",
      "t = 440, loss = -1.1659\n",
      "t = 450, loss = -1.1654\n",
      "t = 460, loss = -1.1652\n",
      "t = 470, loss = -1.1665\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m losses, accs, part_accs \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mtrain(test_model, train_data, loss_fn, optimizer, num_epochs\u001b[38;5;241m=\u001b[39mepochs, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "losses, accs, part_accs = train.train(test_model, train_data, loss_fn, optimizer, num_epochs=epochs, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a54df1-4c4b-4c23-bfa5-43c07a8b2479",
   "metadata": {},
   "source": [
    "## Testing scipt\n",
    "\n",
    "My idea of the testing script. \n",
    "It takes the inverse simulator prediction (Y -> X^)  \n",
    "Pass X^ to mock_simulator to get Y^   \n",
    "evaluate performance from Y to Y^  \n",
    "I don't know if the data is being shuffled at all could contribute too the poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16763cec-b702-4c43-bd3a-1a9dfcb69d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.546716   0.6451244 ]\n",
      " [0.5218257  0.6862989 ]\n",
      " [0.59866196 0.5570911 ]\n",
      " ...\n",
      " [0.         1.0354484 ]\n",
      " [0.2760671  0.48195747]\n",
      " [0.35136002 0.39047256]]\n"
     ]
    }
   ],
   "source": [
    "x_preds = test_model(torch.Tensor(Y))\n",
    "#print(x_preds)\n",
    "mock_simulator = tf.keras.models.load_model('mock_simulator')\n",
    "\n",
    "final_preds = []\n",
    "for i in range(x_preds.shape[0]):\n",
    "    means = x_preds.detach().numpy()[i,:2].T\n",
    "    var = x_preds.detach().numpy()[i,2:].T\n",
    "\n",
    "    varM = np.zeros((2,2))\n",
    "    np.fill_diagonal(varM,var)\n",
    "\n",
    "    pts = np.random.default_rng().multivariate_normal(means, varM, 1).T # get predicted X value \n",
    "    y_hat = mock_simulator(pts.T) # feed back through simulator to get evaluated performance\n",
    "    final_preds.append(y_hat)\n",
    "final_preds = np.array(final_preds).reshape(-1,2)\n",
    "    \n",
    "\n",
    "# for i,d in enumerate(final_preds):\n",
    "#     print(Y[i],d)\n",
    "print(final_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb496032-fc33-4026-9dc8-3dcaccf78ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct = 754 / 3022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30228/4237178316.py:3: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  err = np.divide(a_err, y, out=a_err, where=y==0)\n",
      "/tmp/ipykernel_30228/4237178316.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  err = np.divide(a_err, y, out=a_err, where=y==0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24950363997352745"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_acc(Y,final_preds,margin=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206742c1-3705-4dd8-9c1d-74ebffea17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nll(ytrue, ypreds):\n",
    "    \"\"\"Keras implmementation of multivariate Gaussian negative loglikelihood loss function. \n",
    "    This implementation implies diagonal covariance matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ytrue: tf.tensor of shape [n_samples, n_dims]\n",
    "        ground truth values\n",
    "    ypreds: tf.tensor of shape [n_samples, n_dims*2]\n",
    "        predicted mu and logsigma values (e.g. by your neural network)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    neg_log_likelihood: float\n",
    "        negative loglikelihood averaged over samples\n",
    "        \n",
    "    This loss can then be used as a target loss for any keras model, e.g.:\n",
    "        model.compile(loss=gaussian_nll, optimizer='Adam') \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"ytrue: {ytrue.shape}\")\n",
    "    print(f\"ypreds: {ypreds.shape}\")\n",
    "    n_dims = int(int(ypreds.shape[1])/2)\n",
    "    mu = ypreds[:, 0:n_dims]\n",
    "    logsigma = ypreds[:, n_dims:]\n",
    "    sigma = ypreds[:, n_dims:]\n",
    "    \n",
    "    if K.any(sigma < 0):\n",
    "        print(sigma)\n",
    "        raise ValueError(\"sigma has negative entry/entries\")\n",
    "        \n",
    "    #mse = -0.5*K.sum(K.square((ytrue-mu)/K.exp(logsigma)),axis=1)\n",
    "    #sigma_trace = -K.sum(logsigma, axis=1)\n",
    "    #log2pi = -0.5*n_dims*np.log(2*np.pi)\n",
    "    \n",
    "    log_likelihood = 0.5 * K.log(sigma**2) + (ytrue-mu)**2 / (sigma)\n",
    "\n",
    "    return K.mean(log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5819d-bbfa-4e78-ad68-0ab19bad0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = gaussian_nll\n",
    "\n",
    "# optim = tf.keras.optimizers.Adam(\n",
    "#     learning_rate=0.001)\n",
    "# model.compile(optimizer=optim,\n",
    "#               loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79528927-4cf7-43f0-b6f5-0e917873630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=1000, batch_size= 700, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea823e-81e9-419a-9f6c-9c8cb35b48e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1310b-0987-4941-9c3d-5a33399b4176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
