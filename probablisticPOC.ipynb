{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a42103b-47be-40e3-9d64-13d4595d50c6",
   "metadata": {},
   "source": [
    "# Probablistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678db546-b1d7-43ac-b907-cf30cf664d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import preprocessing\n",
    "from Training import model, utils, dataset, train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defc55b7-af20-44be-90f1-f39f0853dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc(y_hat,y,margin=0.05):\n",
    "    a_err = (np.abs(y_hat - y)) # get normalized error \n",
    "    err = np.divide(a_err, y, out=a_err, where=y==0)\n",
    "    assert(err.shape == y.shape)\n",
    "    num_correct = 0\n",
    "    for row in err:\n",
    "        num_in_row = len(np.where(row < margin)[0]) # margin * 100 because \n",
    "        if num_in_row == len(row):\n",
    "            num_correct += 1\n",
    "\n",
    "    num_samples = y.shape[0]\n",
    "    correct_idx = np.where(err < margin)\n",
    "    num_part_correct = len(correct_idx[0])\n",
    "    num_part_samples = y.shape[0] * y.shape[1]\n",
    "    print(f\"Correct = {num_correct} / {num_samples}\")\n",
    "    return (num_correct/num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7398448-40f9-4c10-afb8-69a8444511c0",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d2858d-a42f-48f6-9de3-d26d88247ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.parseGainAndBWCsv2(\"Data/BW-3000.csv\").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53cbbc10-eb8d-490a-848c-23832f79bc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.200e+02 2.880e-06 3.775e+00 8.280e+10]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7ae70-e420-4c5f-8712-a0dc166fb873",
   "metadata": {},
   "source": [
    "##### Split data into inputs X and target Y Normalize to [0,1] (inclusive) and create Training and Testign Splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd2625d6-6c84-44b0-9b60-68e46eeb41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocessing.MinMaxScaler((0,1)).fit_transform(data)\n",
    "X = data[:,:2]\n",
    "Y = data[:,2:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25750082-6130-442e-929d-67ce5e824bb7",
   "metadata": {},
   "source": [
    "#### Define Model and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c611a-c7f2-4170-94af-4d2025de4f27",
   "metadata": {},
   "source": [
    "##### Define model\n",
    "\n",
    "* Model: MLP with variable hidden layer number and width  \n",
    "* Output layer: \n",
    "    * Node 1 and 2 are the mean for probability mean \n",
    "    * Node 3 and 4 are the diagonal of the covariance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499f1200-7d1d-4910-a2ac-83f2852d4f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_hidden_layers = 5\n",
    "# hidden_layer_size = 500\n",
    "\n",
    "# model = tf.keras.models.Sequential([])\n",
    "\n",
    "# for i in range(num_hidden_layers):\n",
    "#     model.add(tf.keras.layers.Dense(hidden_layer_size, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(4, activation='sigmoid')) # add output layer: node 1,2 are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3618fa3-b062-454c-a941-d2d8783d52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = model.Sigmoid_Net(2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438de8d-1aae-4213-b15a-f70a13b80002",
   "metadata": {},
   "source": [
    "##### Define Loss\n",
    "\n",
    "* Loss: guassian_nll\n",
    "    * cite: https://gist.github.com/sergeyprokudin/4a50bf9b75e0559c1fcd2cae860b879e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "149403d4-2f8c-4238-9cb0-8c7ae7d36a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(test_model.parameters(),lr=0.01)\n",
    "loss_fn = nn.GaussianNLLLoss()\n",
    "\n",
    "dataset1 = dataset.CircuitSynthesisGainAndBandwidthManually(Y, X)\n",
    "train_dataset, val_dataset = utils.splitDataset(dataset1, 0.8)\n",
    "    \n",
    "train_data = DataLoader(train_dataset,batch_size = 500)\n",
    "validation_data = DataLoader(val_dataset, batch_size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8827726e-cbf1-4d93-8aea-51d8bfe01a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 10, loss = -0.7405\n",
      "t = 20, loss = -0.7426\n",
      "t = 30, loss = -0.7426\n",
      "t = 40, loss = -0.7426\n",
      "t = 50, loss = -0.7426\n",
      "t = 60, loss = -0.7426\n",
      "t = 70, loss = -0.7426\n",
      "t = 80, loss = -0.7426\n",
      "t = 90, loss = -0.7426\n",
      "t = 100, loss = -0.7426\n",
      "t = 110, loss = -0.7426\n",
      "t = 120, loss = -0.7426\n",
      "t = 130, loss = -0.7426\n",
      "t = 140, loss = -0.7426\n",
      "t = 150, loss = -0.7426\n",
      "t = 160, loss = -0.8252\n",
      "t = 170, loss = -0.9308\n",
      "t = 180, loss = -0.9431\n",
      "t = 190, loss = -1.0860\n",
      "t = 200, loss = -1.1366\n",
      "t = 210, loss = -1.1588\n",
      "t = 220, loss = -1.1595\n",
      "t = 230, loss = -1.0536\n",
      "t = 240, loss = -1.1294\n",
      "t = 250, loss = -1.1487\n",
      "t = 260, loss = -1.1533\n",
      "t = 270, loss = -1.1561\n",
      "t = 280, loss = -1.1574\n",
      "t = 290, loss = -1.1585\n",
      "t = 300, loss = -1.1596\n",
      "t = 310, loss = -1.1603\n",
      "t = 320, loss = -1.1611\n",
      "t = 330, loss = -1.1619\n",
      "t = 340, loss = -1.1473\n",
      "t = 350, loss = -1.1529\n",
      "t = 360, loss = -1.1590\n",
      "t = 370, loss = -1.1549\n",
      "t = 380, loss = -1.1590\n",
      "t = 390, loss = -1.1576\n",
      "t = 400, loss = -1.1588\n",
      "t = 410, loss = -1.1590\n",
      "t = 420, loss = -1.1590\n",
      "t = 430, loss = -1.1598\n",
      "t = 440, loss = -1.1594\n",
      "t = 450, loss = -1.1606\n",
      "t = 460, loss = -1.1602\n",
      "t = 470, loss = -1.1615\n",
      "t = 480, loss = -1.1610\n",
      "t = 490, loss = -1.1625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m losses, accs, part_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainProbModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/ics-home/Circuit-Synthesis/Training/train.py:50\u001b[0m, in \u001b[0;36mtrainProbModel\u001b[0;34m(model, training_data, loss_fn, optimizer, dtype, num_epochs, print_every)\u001b[0m\n\u001b[1;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     49\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_data):\n\u001b[1;32m     51\u001b[0m     x_var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(x\u001b[38;5;241m.\u001b[39mtype(dtype)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     52\u001b[0m     y_var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(y\u001b[38;5;241m.\u001b[39mtype(dtype)\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:84\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     83\u001b[0m     transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     83\u001b[0m     transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:64\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate([torch\u001b[38;5;241m.\u001b[39mas_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ():  \u001b[38;5;66;03m# scalars\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mas_tensor(batch)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate([\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mas_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ():  \u001b[38;5;66;03m# scalars\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mas_tensor(batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "losses, accs, part_accs = train.trainProbModel(test_model, train_data, loss_fn, optimizer, num_epochs=epochs, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a54df1-4c4b-4c23-bfa5-43c07a8b2479",
   "metadata": {},
   "source": [
    "## Testing scipt\n",
    "\n",
    "My idea of the testing script. \n",
    "It takes the inverse simulator prediction (Y -> X^)  \n",
    "Pass X^ to mock_simulator to get Y^   \n",
    "evaluate performance from Y to Y^  \n",
    "I don't know if the data is being shuffled at all could contribute too the poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16763cec-b702-4c43-bd3a-1a9dfcb69d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 06:13:22.251016: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-29 06:13:22.251066: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-29 06:13:22.251097: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-reevest): /proc/driver/nvidia/version does not exist\n",
      "2022-04-29 06:13:22.251430: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52669483 0.67764723]\n",
      " [0.5254123  0.6803887 ]\n",
      " [0.5595935  0.6230493 ]\n",
      " ...\n",
      " [0.         1.6541357 ]\n",
      " [0.         0.95250136]\n",
      " [0.17390898 0.6078639 ]]\n"
     ]
    }
   ],
   "source": [
    "x_preds = test_model(torch.Tensor(Y))\n",
    "#print(x_preds)\n",
    "mock_simulator = tf.keras.models.load_model('mock_simulator')\n",
    "\n",
    "final_preds = []\n",
    "for i in range(x_preds.shape[0]):\n",
    "    means = x_preds.detach().numpy()[i,:2].T\n",
    "    var = x_preds.detach().numpy()[i,2:].T\n",
    "\n",
    "    varM = np.zeros((2,2))\n",
    "    np.fill_diagonal(varM,var)\n",
    "\n",
    "    pts = np.random.default_rng().multivariate_normal(means, varM, 1).T # get predicted X value \n",
    "    y_hat = mock_simulator(pts.T) # feed back through simulator to get evaluated performance\n",
    "    final_preds.append(y_hat)\n",
    "final_preds = np.array(final_preds).reshape(-1,2)\n",
    "    \n",
    "\n",
    "# for i,d in enumerate(final_preds):\n",
    "#     print(Y[i],d)\n",
    "print(final_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb496032-fc33-4026-9dc8-3dcaccf78ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct = 743 / 3022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30399/4237178316.py:3: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  err = np.divide(a_err, y, out=a_err, where=y==0)\n",
      "/tmp/ipykernel_30399/4237178316.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  err = np.divide(a_err, y, out=a_err, where=y==0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24586366644606222"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_acc(Y,final_preds,margin=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "206742c1-3705-4dd8-9c1d-74ebffea17e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nll(ytrue, ypreds):\n",
    "    \"\"\"Keras implmementation of multivariate Gaussian negative loglikelihood loss function. \n",
    "    This implementation implies diagonal covariance matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ytrue: tf.tensor of shape [n_samples, n_dims]\n",
    "        ground truth values\n",
    "    ypreds: tf.tensor of shape [n_samples, n_dims*2]\n",
    "        predicted mu and logsigma values (e.g. by your neural network)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    neg_log_likelihood: float\n",
    "        negative loglikelihood averaged over samples\n",
    "        \n",
    "    This loss can then be used as a target loss for any keras model, e.g.:\n",
    "        model.compile(loss=gaussian_nll, optimizer='Adam') \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"ytrue: {ytrue.shape}\")\n",
    "    print(f\"ypreds: {ypreds.shape}\")\n",
    "    n_dims = int(int(ypreds.shape[1])/2)\n",
    "    mu = ypreds[:, 0:n_dims]\n",
    "    logsigma = ypreds[:, n_dims:]\n",
    "    sigma = ypreds[:, n_dims:]\n",
    "    \n",
    "    if K.any(sigma < 0):\n",
    "        print(sigma)\n",
    "        raise ValueError(\"sigma has negative entry/entries\")\n",
    "        \n",
    "    #mse = -0.5*K.sum(K.square((ytrue-mu)/K.exp(logsigma)),axis=1)\n",
    "    #sigma_trace = -K.sum(logsigma, axis=1)\n",
    "    #log2pi = -0.5*n_dims*np.log(2*np.pi)\n",
    "    \n",
    "    log_likelihood = 0.5 * K.log(sigma**2) + (ytrue-mu)**2 / (sigma)\n",
    "\n",
    "    return K.mean(log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5819d-bbfa-4e78-ad68-0ab19bad0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = gaussian_nll\n",
    "\n",
    "# optim = tf.keras.optimizers.Adam(\n",
    "#     learning_rate=0.001)\n",
    "# model.compile(optimizer=optim,\n",
    "#               loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79528927-4cf7-43f0-b6f5-0e917873630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=1000, batch_size= 700, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea823e-81e9-419a-9f6c-9c8cb35b48e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1310b-0987-4941-9c3d-5a33399b4176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
