{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Training import model, utils, dataset, train\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import axes3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_1 = model.Model500RELUBN(output_count = 4)\n",
    "# load data 1066 data \n",
    "data1 = utils.parseGainAndBWCsv2('Data/BW-3000.csv')\n",
    "\n",
    "\n",
    "data = np.array(data1).astype(float)\n",
    "#data = np.array(data1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3022, 4)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape(data.shape[0],4)\n",
    "feature = data.T[0]\n",
    "\n",
    "### OLD NORM CODE ###\n",
    "# #ensure norm and denorm work \n",
    "# norm, data_min, data_max = utils.normalize(feature)\n",
    "# feature_1 = utils.denormalize(norm, data_min, data_max)\n",
    "\n",
    "\n",
    "# min_max = dict()\n",
    "# for i in range(4):\n",
    "#     feature = data.T[i]\n",
    "#     norm_feature, data_min, data_max = utils.normalize(feature)\n",
    "#     data.T[i] = norm_feature\n",
    "#     min_max[i] = [data_min, data_max]\n",
    "# norm_data, min_max = utils.normalize(data)\n",
    "# denorm__data = utils.denormalize(norm_data, min_max)\n",
    "### END OLD NORM CODE ###\n",
    "\n",
    "norm_data, min_max = utils.normalize(data)\n",
    "denorm_data = utils.denormalize(norm_data, min_max)\n",
    "data = norm_data.reshape(norm_data.shape[0],2,2)\n",
    "\n",
    "#data = data.reshape(data.shape[0],2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data[:, 1]\n",
    "data_y = data[: ,0]\n",
    "data_set = dataset.CircuitSynthesisGainAndBandwidthManually(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = utils.splitDataset(data_set, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "loss_fn = nn.GaussianNLLLoss().type(dtype)  # loss can be changed here. This is the first one i tried that gave meaningful results\n",
    "\n",
    "x = optim.Adam\n",
    "optimizer1 = x(test_model_1.parameters(), lr=3e-4)#0.06448295999961791)  # TODO haven't experimented with this yet\n",
    "train_data = DataLoader(train_dataset, batch_size=250)\n",
    "test_data = DataLoader(val_dataset, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train = len(train_dataset)\n",
    "total_test = len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    assert data.shape[1] == 4, \"reshape the data first to (-1, 4)\"\n",
    "    normed_data = np.zeros(data.shape)\n",
    "    normers = []\n",
    "    for i in range(len(data.T)):\n",
    "        normer = preprocessing.MinMaxScaler((-1,1))\n",
    "        normed_data.T[i] = normer.fit_transform(data.T[i].reshape(-1,1)).reshape(-1)\n",
    "        normers.append(normer)\n",
    "    return normed_data,np.array(normers)\n",
    "\n",
    "def denormalize(normed_data,normers):\n",
    "    denormed_data = np.zeros(normed_data.shape)\n",
    "    for i in range(len(normed_data.T)):\n",
    "        denormed_data.T[i] = normers[i].inverse_transform(normed_data.T[i].reshape(-1,1)).reshape(-1)\n",
    "    return denormed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(model, loader, min_max, margin, dtype=torch.FloatTensor, verbose = True):\n",
    "    num_part_correct = 0\n",
    "    num_part_samples = 0\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    model.eval()  # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    loss_list = []\n",
    "    all_preds = []\n",
    "    for x, y in loader:\n",
    "        with torch.no_grad():\n",
    "            x_var = torch.autograd.Variable(x.type(dtype))\n",
    "\n",
    "        y_hat = model(x_var)\n",
    "        \n",
    "        y_hat = np.array(y_hat.detach())\n",
    "        var1_expectation = y_hat[:,0]\n",
    "        var2_expectation = y_hat[:,1]\n",
    "        var1_std = np.sqrt(np.absolute(y_hat[:,2]))\n",
    "        var2_std = np.sqrt(np.absolute(y_hat[:,3]))\n",
    "        \n",
    "        var1 = np.random.normal(var1_expectation, var1_std)[:,None]\n",
    "        var2 = np.random.normal(var2_expectation, var2_std)[:,None]\n",
    "        y_hat = np.concatenate((var1,var2), axis = 1)\n",
    "\n",
    "        y_min_max = min_max[2:] # discard x min_maxs\n",
    "        y = denormalize(y,y_min_max)\n",
    "        y_hat = denormalize(y_hat,y_min_max)\n",
    "       \n",
    "        err = np.abs(y_hat - y)/y\n",
    "        for row in err:\n",
    "            num_in_row = len(np.where(row < margin)[0])\n",
    "            if num_in_row == len(row):\n",
    "                num_correct += 1\n",
    "\n",
    "        num_samples += y.shape[0]\n",
    "        correct_idx = np.where(err < margin)\n",
    "        num_part_correct += len(correct_idx[0])\n",
    "        num_part_samples += y.shape[0] * y.shape[1]\n",
    "        \n",
    "        all_preds.extend(y_hat)\n",
    "    part_acc = float(num_part_correct) / num_part_samples\n",
    "    acc = float(num_correct) / num_samples\n",
    "    if verbose:\n",
    "        print('Got %d / %d partially correct (%.2f pct)' % (num_part_correct, num_part_samples, 100 * part_acc)) \n",
    "        print('Got %d / %d correct (%.2f pct)' % (num_correct, num_samples, 100 * acc)) \n",
    "    return acc, part_acc, all_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 1, loss = 2.8643\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'check_gaussian_raw_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-bb91ef65735f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mtacc5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_guassian_raw_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_model_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mtacc10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_guassian_raw_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_model_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mtacc20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_gaussian_raw_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_model_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mta5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtacc5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'check_gaussian_raw_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "ta5 = []\n",
    "ta10 = []\n",
    "ta20 = []\n",
    "va5 = []\n",
    "va10 = []\n",
    "va20 = []\n",
    "train_loss = []\n",
    "for epoch in range(500):\n",
    "\n",
    "    avg_loss = 0\n",
    "        \n",
    "    test_model_1.train()\n",
    "    for t, (x, y) in enumerate(train_data):\n",
    "\n",
    "        x_var = torch.autograd.Variable(x.type(dtype))\n",
    "        y_var = torch.autograd.Variable(y.type(dtype).float())\n",
    "\n",
    "        # make predictions\n",
    "        output = test_model_1(x_var)\n",
    "        \n",
    "        expectation_1 = output[:,0]\n",
    "        expectation_2 = output[:,1]\n",
    "        var_1 = torch.absolute(output[:,2])\n",
    "        var_2 = torch.absolute(output[:,3])\n",
    "        loss1 = loss_fn(expectation_1, y_var[:,0], var_1)\n",
    "        loss2 = loss_fn(expectation_2, y_var[:,1], var_2)\n",
    "        avg_loss += (loss1.item() + loss2.item() - avg_loss) / (t+1)\n",
    "        loss = loss1 + loss2\n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "    \n",
    "    print('t = %d, loss = %.4f' % (epoch + 1, avg_loss))\n",
    "    train_loss.append(avg_loss)\n",
    "    vacc5, _,_ = a(test_model_1, test_data, min_max, .05, verbose=False)\n",
    "    vacc10, _,_ = a(test_model_1, test_data, min_max, .1, verbose=False)\n",
    "    vacc20, _,_ = a(test_model_1, test_data, min_max, .2, verbose=False)\n",
    "    tacc5, _,_ = a(test_model_1, train_data, min_max, .05, verbose=False)\n",
    "    tacc10, _,_ = a(test_model_1, train_data, min_max, .1, verbose=False)\n",
    "    tacc20, _,_ = a(test_model_1, train_data, min_max, .2, verbose=False)\n",
    "    \n",
    "    ta5.append(tacc5)\n",
    "    ta10.append(tacc10)\n",
    "    ta20.append(tacc20)\n",
    "    va5.append(vacc5)\n",
    "    va10.append(vacc10)\n",
    "    va20.append(vacc20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,5))\n",
    "plt.plot(ta5, color='grey', label='5 percent margin')\n",
    "plt.plot(ta10, color='blue', label = '10 percent margin')\n",
    "plt.plot(ta20, color='red', label = '20 percent margin')\n",
    "plt.legend(title = 'accuracy plot training batch norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(va5, color='grey', label='5 percent margin')\n",
    "plt.plot(va10, color='blue', label = '10 percent margin')\n",
    "plt.plot(va20, color='red', label = '20 percent margin')\n",
    "plt.legend(title = 'accuracy plot validation batch norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(train_loss, color='grey', label='train loss')\n",
    "plt.legend(title = 'training loss without batch norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(va5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(ta5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each data point will be a tuple with a format of (Transistor width, resistor load, Bandwidth, Gain)\n",
    "k = data_set[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=k[0],k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(y[None,:]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_hat = test_model_1(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n = utils.denormalize(y,min_max[2:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n = utils.denormalize(x[None,:],min_max[0:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = utils.denormalize(y_hat.detach().numpy(),min_max[2:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simulator(width, resistor):\n",
    "    gain = 20 * math.log(abs(-1 * resistor * (49 * math.pow(10, -3) * 0.02302 * width / math.pow(10, -6) * 0.2)),10)\n",
    "    bandwidth = 1/(2 * math.pi * resistor * 348.435)\n",
    "    return gain, bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_simulator(x_n[0,0], x_n[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
